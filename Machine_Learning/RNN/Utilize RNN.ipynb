{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Utilize RNN.ipynb","provenance":[],"authorship_tag":"ABX9TyPfWEPocmaagTvIlnxHd8lp"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"d5WJ40iaI5df"},"source":["# 순환 신경망 활용\n","\n","# 목차\n","\n","# 임베딩<sup>Embedding</sup>\n","\n","## 임베딩<sup>Embedding</sup>이란?\n","컴퓨터가 이해하는 벡터로 변경하는 것\n","- 단어 수준 임베딩  \n","  - 동음이의어를 구별 가능\n","  - Word2Vec, FastText\n","- 문장 수준 임베딩\n","  - 문장의 앞뒤를 보고 이를 파악할 수 있음\n","  - Elmo, Bert, GPT"]},{"cell_type":"markdown","metadata":{"id":"OQ-Jcuu7PdmO"},"source":["간단한 단어 수준 임베딩에는 희소 행렬을 결과로 출력하는 원-핫 인코딩<sup>one-hot encoding</sup>이 있고,  \n","이와 반대로는 자원의 낭비가 적고 단어 사이의 유사도를 알 수 있는 밀집 행렬로 단어를 표현할 수 있다."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0MSdjnH8YI_K","executionInfo":{"status":"ok","timestamp":1627429755789,"user_tz":-540,"elapsed":2369,"user":{"displayName":"김재성","photoUrl":"","userId":"07271719604473563567"}},"outputId":"4b0059eb-a7c9-4a20-9382-d2f37e7f27a3"},"source":["# Keras 라이브러리를 이용하여 밀집 행렬로 단어 수준 임베딩하기\n","# 라이브러리 불러오기\n","import tensorflow as tf\n","\n","# 임베딩 레이어\n","embedding_layer = tf.keras.layers.Embedding(100, 3)\n","result = embedding_layer(tf.constant([12, 8, 15, 20]))\n","print(result)"],"execution_count":2,"outputs":[{"output_type":"stream","text":["tf.Tensor(\n","[[ 0.01936314  0.02011268  0.00141933]\n"," [-0.01075004  0.03470894  0.03640759]\n"," [-0.03879552  0.0181275  -0.02100657]\n"," [ 0.01774385 -0.03633636  0.0420985 ]], shape=(4, 3), dtype=float32)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"0NuNOlD5Y7P4"},"source":["Keras 에서 제공하는 Embedding Layer\n","  - input_dim : 단어 사전의 크기\n","  - output_dim : 출력 벡터의 크기\n","  - input_length : 입력 길이 ( = 문장 내 단어의 개수)"]},{"cell_type":"code","metadata":{"id":"u6pSWVGKfth3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1627430177721,"user_tz":-540,"elapsed":363,"user":{"displayName":"김재성","photoUrl":"","userId":"07271719604473563567"}},"outputId":"2b7a781d-d7e0-4bb5-b903-253c7d5e7926"},"source":["# 임베딩 레이어를 활용한 RNN\n","model = tf.keras.Sequential()\n","model.add(tf.keras.layers.Embedding(100, 1, input_length=32))\n","model.add(tf.keras.layers.LSTM(units=32))\n","model.add(tf.keras.layers.Dense(units=1))\n","model.summary()"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Model: \"sequential_1\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","embedding_2 (Embedding)      (None, 32, 1)             100       \n","_________________________________________________________________\n","lstm_1 (LSTM)                (None, 32)                4352      \n","_________________________________________________________________\n","dense_1 (Dense)              (None, 1)                 33        \n","=================================================================\n","Total params: 4,485\n","Trainable params: 4,485\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"564xo1EMfb4K"},"source":["라이브러리 선 호출 방식"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jQMP6G6NgNSf","executionInfo":{"status":"ok","timestamp":1627431640903,"user_tz":-540,"elapsed":349,"user":{"displayName":"김재성","photoUrl":"","userId":"07271719604473563567"}},"outputId":"2ee4075f-c056-40e1-8c6b-2b389aa0d002"},"source":["import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Embedding, LSTM, Dense\n","\n","# 임베딩 레이어 활용\n","model = Sequential()\n","model.add(Embedding(100, 3, input_length = 32))\n","model.add(LSTM(32))\n","model.add(Dense(1))\n","model.summary()"],"execution_count":6,"outputs":[{"output_type":"stream","text":["Model: \"sequential_3\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","embedding_3 (Embedding)      (None, 32, 3)             300       \n","_________________________________________________________________\n","lstm_2 (LSTM)                (None, 32)                4608      \n","_________________________________________________________________\n","dense_2 (Dense)              (None, 1)                 33        \n","=================================================================\n","Total params: 4,941\n","Trainable params: 4,941\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"_d6vJrKre6gM"},"source":["# 양방향<sup>Bidirectional</sup> RNN\n","자연어 데이터의 경우 순서대로 데이터를 처리하고, 역순으로도 처리할 경우 더 좋은 성능을 발휘하기도 한다."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"698-MwjdgEK8","executionInfo":{"status":"ok","timestamp":1627432038398,"user_tz":-540,"elapsed":855,"user":{"displayName":"김재성","photoUrl":"","userId":"07271719604473563567"}},"outputId":"cdd4d2ad-45ff-49f5-844f-65ea4e76b358"},"source":["# Bidirectional LSTM\n","from tensorflow.keras.layers import Bidirectional\n","\n","model = Sequential()\n","model.add(Embedding(100, 3))\n","model.add(Bidirectional(LSTM(32)))\n","model.add(Dense(1))\n","\n","model.summary()"],"execution_count":10,"outputs":[{"output_type":"stream","text":["Model: \"sequential_7\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","embedding_6 (Embedding)      (None, None, 3)           300       \n","_________________________________________________________________\n","bidirectional_2 (Bidirection (None, 64)                9216      \n","_________________________________________________________________\n","dense_5 (Dense)              (None, 1)                 65        \n","=================================================================\n","Total params: 9,581\n","Trainable params: 9,581\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"05u57K6jgQS4"},"source":["# 스태킹 RNN\n","RNN layer를 여러 층 쌓기 위해서는 기본적으로 마지막 상태 값만 출력하는 RNN layer의 옵션을 변경해야한다.\n"," "]},{"cell_type":"code","metadata":{"id":"TSm8xsYfiJ-W","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1627432594762,"user_tz":-540,"elapsed":972,"user":{"displayName":"김재성","photoUrl":"","userId":"07271719604473563567"}},"outputId":"9cf6f3d9-819b-482d-ccc5-2c67e7544039"},"source":["# 스태킹 RNN 예제\n","model = Sequential()\n","model.add(Embedding(100, 32))\n","model.add(LSTM(32, return_sequences=True))\n","model.add(LSTM(32))\n","model.add(Dense(1))\n","model.summary()"],"execution_count":12,"outputs":[{"output_type":"stream","text":["Model: \"sequential_9\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","embedding_8 (Embedding)      (None, None, 32)          3200      \n","_________________________________________________________________\n","lstm_8 (LSTM)                (None, None, 32)          8320      \n","_________________________________________________________________\n","lstm_9 (LSTM)                (None, 32)                8320      \n","_________________________________________________________________\n","dense_7 (Dense)              (None, 1)                 33        \n","=================================================================\n","Total params: 19,873\n","Trainable params: 19,873\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"x4oAltIVifTI"},"source":["# 순환 드롭아웃\n","내장 RNN은 순환 상태의 드롭아웃 비율을 뜻하는 recurrent_dropout을 제공하고 있다.  \n","0과 1사이의 부동 소수점이고 과대적합을 방지하기 위한 기능이다."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PZcYf5oljRkC","executionInfo":{"status":"ok","timestamp":1627432834620,"user_tz":-540,"elapsed":322,"user":{"displayName":"김재성","photoUrl":"","userId":"07271719604473563567"}},"outputId":"8c593a54-d625-47cc-9e8f-f922a782ef74"},"source":["# 순환 드롭아웃\n","model = Sequential()\n","model.add(Embedding(100, 32))\n","model.add(LSTM(32, recurrent_dropout=0.2, dropout=0.2))\n","model.add(Dense(1, activation='sigmoid'))\n","model.summary()"],"execution_count":14,"outputs":[{"output_type":"stream","text":["Model: \"sequential_11\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","embedding_9 (Embedding)      (None, None, 32)          3200      \n","_________________________________________________________________\n","lstm_10 (LSTM)               (None, 32)                8320      \n","_________________________________________________________________\n","dense_8 (Dense)              (None, 1)                 33        \n","=================================================================\n","Total params: 11,553\n","Trainable params: 11,553\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ly6OAktojW0c"},"source":[""],"execution_count":null,"outputs":[]}]}